{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First cell - Setup paths\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Setup paths as before\n",
    "notebook_dir = os.path.dirname('__file__')\n",
    "lerobot_root = os.path.abspath(os.path.join(notebook_dir, '../..'))\n",
    "sys.path.append(lerobot_root)\n",
    "os.chdir(lerobot_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[15640]: Class AVFFrameReceiver is implemented in both /Users/shreyas/opt/anaconda3/envs/caferacer/lib/python3.10/site-packages/av/.dylibs/libavdevice.61.3.100.dylib (0x31cf0c798) and /Users/shreyas/opt/anaconda3/envs/caferacer/lib/libavdevice.59.7.100.dylib (0x325278778). One of the two will be used. Which one is undefined.\n",
      "objc[15640]: Class AVFAudioReceiver is implemented in both /Users/shreyas/opt/anaconda3/envs/caferacer/lib/python3.10/site-packages/av/.dylibs/libavdevice.61.3.100.dylib (0x31cf0c7e8) and /Users/shreyas/opt/anaconda3/envs/caferacer/lib/libavdevice.59.7.100.dylib (0x3252787c8). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Optional, Dict, Union, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import modal\n",
    "import gc\n",
    "import json\n",
    "from collections import deque\n",
    "import time\n",
    "import re\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset\n",
    "from lerobot.caferacer.scripts.image_utils import reorder_tensor_dimensions, tensor_to_pil, display_images\n",
    "from lerobot.caferacer.scripts.aug_utils import flip_frame, apply_color, get_mask, precompute_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "train_repo = \"shreyasgite/so100_base_left\"\n",
    "eval_repo = \"shreyasgite/eval_test\"\n",
    "MODEL_ID = \"gemini-2.0-flash\"  # Use Gemini 2.0 Flash for 3D capabilities\n",
    "PRO_MODEL_ID ='gemini-2.0-pro-exp-02-05'\n",
    "#repo_id0 = \"shreyasgite/so100_base_env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreyas/opt/anaconda3/envs/caferacer/lib/python3.10/asyncio/events.py:80: DeprecationError: 2025-01-27: `modal.Function.lookup` is deprecated and will be removed in a future release. It can be replaced with `modal.Function.from_name`.\n",
      "\n",
      "See https://modal.com/docs/guide/modal-1-0-migration for more information.\n",
      "  self._context.run(self._callback, *self._args)\n",
      "/Users/shreyas/opt/anaconda3/envs/caferacer/lib/python3.10/asyncio/events.py:80: DeprecationError: 2025-02-11: Looking up class methods using Function.from_name will be deprecated in a future version of Modal.\n",
      "Use modal.Cls.from_name instead, e.g.\n",
      "\n",
      "GroundedSam = modal.Cls.from_name(\"grounded-sam\", \"GroundedSam\")\n",
      "instance = GroundedSam(...)\n",
      "instance.run.remote(...)\n",
      "\n",
      "  self._context.run(self._callback, *self._args)\n"
     ]
    }
   ],
   "source": [
    "gsam = modal.Function.lookup(\"grounded-sam\",\"GroundedSam.run\", environment_name='prod')\n",
    "#gsam = modal.Cls.from_name(\"grounded-sam\",\"GroundedSam.run\", environment_name='prod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_scene(img, prompt=None) -> str:\n",
    "    \"\"\"Prompts Gemini 2.0 Flash for scene analysis (3D bounding boxes, orientation).\"\"\"\n",
    "    if prompt is None:\n",
    "        prompt = \"\"\"Describe the scene from the top view. Focus on objects, positions, and spatial relationships. \n",
    "        Point to no more than 10 items in the image. Include following items in the analysis: Robot arm, container, and Lego bricks.\n",
    "        The answer should follow the json format: [{\"point\": <point>, \"label\": <label1>, \"description\": <description>}, ...]. The points are in [y, x] format normalized to 0-1000. One element a line.\n",
    "        \"\"\"\n",
    "    response = client.models.generate_content(\n",
    "        model=MODEL_ID,\n",
    "        contents=[img, prompt],\n",
    "        config=types.GenerateContentConfig(temperature=0.1),\n",
    "    )\n",
    "    return response.text\n",
    "    \n",
    "def analyze_multi_view(img_0, img_1, prompt=None, context=None) -> str:\n",
    "    if prompt is None:\n",
    "        prompt = \"\"\"Given the top view image and analysis, get additional details from the front view image. \n",
    "        Provide a combined analysis of the top and front views. Focus on heights, occlusions, and depth relationships.\n",
    "        The answer should follow the json format: [{\"point\": <point>, \"label\": <label1>, \"description\": <description>}, ...]. The points are in [y, x] format normalized to 0-1000. One element a line.\n",
    "        \"\"\"\n",
    "    response = client.models.generate_content(\n",
    "        model=MODEL_ID,\n",
    "        contents=[img_0, prompt, context, img_1],\n",
    "        config=types.GenerateContentConfig(temperature=0.1),\n",
    "    )\n",
    "    return response.text\n",
    "\n",
    "def get_summary(analysis_data: List[Dict], prompt_template: str) -> str:\n",
    "    \"\"\"Prompts Gemini (text model) for a summary of the analysis.\"\"\"\n",
    "    # Flatten the analysis data into a single string (for text-based Gemini)\n",
    "    combined_analysis = \"\\n\".join([str(episode) for episode in analysis_data])\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=PRO_MODEL_ID, # Use this or something similar for text generation\n",
    "            contents=[prompt_template.format(combined_analysis)],\n",
    "            config = types.GenerateContentConfig(temperature=0.4, top_p=0.8)\n",
    "        )\n",
    "    except ServerError:\n",
    "        response = client.models.generate_content(\n",
    "            model=MODEL_ID, # Use this or something similar for text generation\n",
    "            contents=[prompt_template.format(combined_analysis)],\n",
    "            config = types.GenerateContentConfig(temperature=0.4, top_p=0.8)\n",
    "        )\n",
    "\n",
    "    return response.text\n",
    "\n",
    "def parse_json(response_text):\n",
    "    # Parsing out the markdown fencing\n",
    "    # Find JSON content within triple backticks if present\n",
    "    try:\n",
    "        # Remove markdown code block formatting, if present\n",
    "        response_text = response_text.strip().replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "        data = json.loads(response_text)\n",
    "        return data\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON parsing error: {e}\")\n",
    "        print(f\"Raw response: {response_text}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Initial Training Data Analysis ---\n",
    "\n",
    "def analyze_training_data(repo_id: str, episodes: List[int] = None) -> List[Dict]:\n",
    "    \"\"\"Analyzes the first frames of each training episode.\"\"\"\n",
    "    dataset = LeRobotDataset(repo_id, episodes=episodes) #Load initial dataset\n",
    "    num_episodes = dataset.num_episodes\n",
    "    all_episodes_analysis = []\n",
    "\n",
    "    for ep_idx in tqdm(range(num_episodes), desc=\"Analyzing Training Episodes\"):\n",
    "        from_idx = dataset.episode_data_index[\"from\"][ep_idx].item()\n",
    "        to_idx = dataset.episode_data_index[\"to\"][ep_idx].item()\n",
    "        \n",
    "        top_frame = dataset[from_idx][\"observation.images.phone\"]\n",
    "        front_frame = dataset[from_idx][\"observation.images.laptop\"]\n",
    "\n",
    "        top_image = tensor_to_pil(top_frame)\n",
    "        front_image = tensor_to_pil(front_frame)\n",
    "\n",
    "        top_analysis_raw = analyze_scene(top_image)\n",
    "        analysis = analyze_multi_view(top_image, front_image, context=top_analysis_raw)\n",
    "        \n",
    "        analysis = parse_json(analysis)\n",
    "\n",
    "        all_episodes_analysis.append({\"episode\": ep_idx, \"analysis\": analysis})\n",
    "        #all_episodes_analysis.append(\n",
    "        #    {\"episode\": ep_idx, \"top\": top_analysis, \"front\": front_analysis}\n",
    "        #)\n",
    "\n",
    "    return all_episodes_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_training_data(analysis_data: List[Dict]) -> str:\n",
    "    \"\"\"Generates a summary of the training data analysis.\"\"\"\n",
    "    summary_prompt_template = \"\"\"\n",
    "    Analyze the following dataset of robot pick-and-place episodes with lego bricks and containers.\n",
    "    Summarize patterns, biases, and limitations in the training data.\n",
    "        \n",
    "    Focus on:\n",
    "    1. Object distributions (positions, orientations, colors)\n",
    "    2. Success patterns\n",
    "    3. Potential biases (e.g., container always on left)\n",
    "    4. Limitations in the dataset diversity\n",
    "\n",
    "    {0}\n",
    "\n",
    "    Provide the following:\n",
    "    1.  **Training Statistics:**  Quantify key aspects, such as the percentage of episodes\n",
    "        where the container is on the left vs. right side of the robot.\n",
    "    2.  **Potential Biases:** Identify any biases in the data (e.g., only one container color).\n",
    "    3.  **Suggestions for Data Augmentation:**  Suggest specific augmentations to address\n",
    "        biases and improve generalization. Categorize suggestions like this:\n",
    "        - flip_frame:  (If the data is biased towards one side)\n",
    "        - change_color: object-container:color-yellow  (If container color variety is needed)\n",
    "        - inpaint_distraction: List_of_distraction_objects:[object1, object2] (If distractions should be removed)\n",
    "    \"\"\"\n",
    "    return get_summary(analysis_data, summary_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_episode_success(\n",
    "        #first_frame: Union[Image.Image, torch.Tensor],\n",
    "        last_frame: Union[Image.Image, torch.Tensor]\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate whether an episode was successful by comparing first and last frames.\n",
    "        \n",
    "        Args:\n",
    "            first_frame: First frame of the episode\n",
    "            last_frame: Last frame of the episode\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with success evaluation and reasoning\n",
    "        \"\"\"\n",
    "        prompt = \"\"\"\n",
    "        Analyze the final state of this robotics task (picking and placing Lego bricks).\n",
    "        1.  Is the Lego brick inside the container? (Answer with YES or NO).\n",
    "        2.  If NO, provide a concise reason for the failure, relating it to the scene.\n",
    "        Output should be JSON in format: {\"success\": bool, \"failure_reason\": str}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert tensors to PIL if needed\n",
    "        #if isinstance(first_frame, torch.Tensor):\n",
    "        #    first_frame = tensor_to_pil(first_frame)\n",
    "        if isinstance(last_frame, torch.Tensor):\n",
    "            last_frame = tensor_to_pil(last_frame)\n",
    "            \n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=MODEL_ID,\n",
    "                contents=[last_frame, prompt],\n",
    "                config=types.GenerateContentConfig(temperature=0.2)\n",
    "            )\n",
    "            \n",
    "            \n",
    "            return response.text\n",
    "            \n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating episode success: {e}\")\n",
    "            return {\"error\": str(e), \"success\": False, \"confidence\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_eval_data(repo_id: str, episodes: List[int] = None, GPU_POOR=True) -> List[Dict]:\n",
    "    \"\"\"Analyzes the first frames of each training episode.\"\"\"\n",
    "    dataset = LeRobotDataset(repo_id, episodes=episodes) #Load initial dataset\n",
    "    num_episodes = dataset.num_episodes\n",
    "    all_episodes_analysis = []\n",
    "\n",
    "    for ep_idx in tqdm(range(num_episodes), desc=\"Analyzing Eval Episodes\"):\n",
    "        from_idx = dataset.episode_data_index[\"from\"][ep_idx].item()\n",
    "        to_idx = dataset.episode_data_index[\"to\"][ep_idx].item()\n",
    "        \n",
    "        top_first = dataset[from_idx][\"observation.images.phone\"]\n",
    "        front_first = dataset[from_idx][\"observation.images.laptop\"]\n",
    "        top_image = tensor_to_pil(top_first)\n",
    "        front_image = tensor_to_pil(front_first)\n",
    "\n",
    "        top_analysis_raw = analyze_scene(top_image)\n",
    "        scene_analysis_raw = analyze_multi_view(top_image, front_image, context=top_analysis_raw)\n",
    "        scene_analysis = parse_json(scene_analysis_raw)\n",
    "        \n",
    "        # TODO: Add front view to the eval analysis\n",
    "        top_last = dataset[to_idx-1][\"observation.images.phone\"]\n",
    "        eval_analysis_raw = evaluate_episode_success(top_last)\n",
    "        eval_analysis = parse_json(eval_analysis_raw)\n",
    "\n",
    "        all_episodes_analysis.append(\n",
    "            {\"episode\": ep_idx, \"episode_eval\": eval_analysis, \"scene_analysis\": scene_analysis}\n",
    "        )\n",
    "        \n",
    "    return all_episodes_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmentations(\n",
    "    evaluation_results: List[Dict], training_summary: str\n",
    ") -> str:\n",
    "    \"\"\"Summarizes evaluation results and suggests augmentations.\"\"\"\n",
    "\n",
    "    # Combine evaluation results into a string format\n",
    "    combined_eval_results = \"\"\n",
    "    for result in evaluation_results:\n",
    "        combined_eval_results += f\"Episode {result['episode']}: \\n\"\n",
    "        combined_eval_results += f\"Scene Description: {result['scene_analysis']} \\n\"\n",
    "        combined_eval_results += f\"Episode Evaluation: {result['episode_eval']} \\n\"\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    Based on the training data summary and failed evaluation episodes,\n",
    "    suggest data augmentations to improve the robot's policy.\n",
    "        \n",
    "    Consider the following types of augmentations:\n",
    "    1. flip_frame: If the training data shows position bias (e.g., container always on left)\n",
    "    2. change_color: If the training data shows color bias (e.g., container always blue)\n",
    "    3. inpaint_distraction: If distractions in scenes affect performance. Distractions are objects that are not the target object.\n",
    "    Format the response as JSON with the following structure:\n",
    "        {\n",
    "            \"recommended_augmentations\": {\n",
    "                    'flip_frame': True, \n",
    "                    'change_color': {\n",
    "                        'object': 'blue container', \n",
    "                        'target_color': 'blue'\n",
    "                    },\n",
    "                    'inpaint_distraction': {\n",
    "                        'distraction_objects': ['object1', 'object2']\n",
    "                    }\n",
    "            },\n",
    "            \"reasoning\": \"overall explanation of recommendations\",\n",
    "            \"expected_improvements\": \"how these changes should help\"\n",
    "        }\n",
    "    \"\"\"\n",
    "    prompt += f\"Here is a summary of the training data used for the initial policy: {training_summary}\"\n",
    "    prompt += f\"Here are the evaluation results: {combined_eval_results}\"\n",
    "    response = client.models.generate_content(\n",
    "        model=PRO_MODEL_ID,\n",
    "        contents=[prompt],\n",
    "        config=types.GenerateContentConfig(temperature=0.2)\n",
    "    )\n",
    "    \n",
    "    return response.text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4f34d562e042cda6d3d71c95c069b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyzing Training Episodes:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_results = analyze_training_data(train_repo, episodes=list(range(8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_summary_1 = summarize_training_data(train_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's an analysis of the provided robot pick-and-place dataset, focusing on the requested aspects:\n",
      "\n",
      "**1. Training Statistics:**\n",
      "\n",
      "*   **Container Position:**  In all episodes (100%), the container is described as being on the *right* side of the robot arm or image.  There are no instances of the container being on the left.\n",
      "*   **Lego Brick Position:** In all episodes (100%), the Lego bricks are consistently described as being on the *left* side of the robot arm or image.\n",
      "*   **Lego Brick Colors:** The Lego bricks are primarily described as green, yellow, gray, and black.\n",
      "*   **Container Color:** The container is consistently described as blue.\n",
      "*   **Robot Arm Color:** The robot arm is consistently described as red, often with black components.\n",
      "\n",
      "**2. Potential Biases:**\n",
      "\n",
      "*   **Strong Positional Bias:** The most significant bias is the consistent placement of the container on the right and the Lego bricks on the left.  A robot trained solely on this data would likely fail to generalize to scenarios where the container is on the left or the bricks are on the right.\n",
      "*   **Limited Color Variation (Container):**  The container is always blue.  This lack of color diversity could hinder the robot's ability to recognize containers of different colors.\n",
      "*   **Limited Color Variation (Bricks):** While there's some variation (green, yellow, gray, black), it's not extensive.  The robot might struggle with bricks of significantly different colors (e.g., red, purple, white).\n",
      "*   **Consistent Robot Arm Color:** The robot arm is always red. While less critical than the container/brick biases, this could theoretically impact performance if the robot were deployed in an environment with a different colored arm.\n",
      "*   **Consistent Background:** The background is not explicitly described in detail, but the presence of shadows and mentions of a \"table\" suggest a relatively consistent, uncluttered environment.\n",
      "\n",
      "**3. Suggestions for Data Augmentation:**\n",
      "\n",
      "To address these biases and improve generalization, I recommend the following data augmentation strategies:\n",
      "\n",
      "*   **`flip_frame: horizontal`**:  This is crucial.  By horizontally flipping the images (and correspondingly adjusting the bounding box coordinates), we can effectively simulate placing the container on the left and the bricks on the right, breaking the dominant positional bias.\n",
      "\n",
      "*   **`change_color: object-container: color-yellow`**:  Change the container's color to yellow.\n",
      "*   **`change_color: object-container: color-green`**:  Change the container's color to green.\n",
      "*   **`change_color: object-container: color-red`**:  Change the container's color to red.\n",
      "*   **`change_color: object-container: color-gray`**: Change the container's color to gray.\n",
      "\n",
      "    These color changes will diversify the container's appearance.\n",
      "\n",
      "*   **`change_color: object-Lego bricks: color-red`**: Change some Lego bricks to red.\n",
      "*   **`change_color: object-Lego bricks: color-blue`**: Change some Lego bricks to blue.\n",
      "*   **`change_color: object-Lego bricks: color-white`**: Change some Lego bricks to white.\n",
      "*   **`change_color: object-Lego bricks: color-purple`**: Change some Lego bricks to purple.\n",
      "\n",
      "    These color changes will diversify the lego bricks' appearance.\n",
      "\n",
      "*   **`add_distraction: object-random: count-1`**: Add a single, randomly placed object (e.g., a small toy, a tool, a different colored block) to the scene.  This helps the robot learn to ignore irrelevant objects.\n",
      "*   **`add_distraction: object-random: count-2`**: Increase the number of random distraction objects to two.\n",
      "*   **`add_distraction: object-random: count-3`**: Increase the number of random distraction objects to three.\n",
      "\n",
      "*   **`rotate_scene: angle-15`**: Rotate the entire scene by 15 degrees.\n",
      "*   **`rotate_scene: angle--15`**: Rotate the entire scene by -15 degrees.\n",
      "*   **`rotate_scene: angle-30`**: Rotate the entire scene by 30 degrees.\n",
      "*   **`rotate_scene: angle--30`**: Rotate the entire scene by -30 degrees.\n",
      "\n",
      "    These rotations introduce variations in object orientation.\n",
      "\n",
      "*   **`change_background: background-cluttered`**: Replace the simple background with a more cluttered one (e.g., a workshop, a shelf with various objects). This is a more advanced augmentation that significantly increases the challenge.\n",
      "*   **`change_background: background-plain_white`**: Change to a plain white background.\n",
      "*   **`change_background: background-plain_gray`**: Change to a plain gray background.\n",
      "\n",
      "* **`inpaint_distraction: List_of_distraction_objects:[wire]`**: Remove the wire from some of the images. This forces the robot to focus on the core task-relevant objects.\n",
      "* **`inpaint_distraction: List_of_distraction_objects:[shadow]`**: Remove the shadows from some of the images.\n",
      "\n",
      "**4. Limitations in Dataset Diversity:**\n",
      "\n",
      "Besides the biases, the dataset has these limitations:\n",
      "\n",
      "*   **Small Number of Episodes:** Only 8 episodes are provided.  A robust real-world application would require hundreds or thousands of episodes.\n",
      "*   **Single Container Shape:** The container is always described as \"square.\"  Different container shapes (rectangular, circular) should be included.\n",
      "*   **Single Robot Type:**  The dataset only features one type of robot arm.  Different arm configurations and end-effectors would improve generalization.\n",
      "*   **No Failed Pick-and-Place Attempts:** The descriptions don't indicate whether the robot successfully picked and placed the bricks.  Including examples of failures (e.g., dropped bricks, incorrect placement) is crucial for training a robust system.\n",
      "* **No Occlusion:** There is no mention of the lego bricks being occluded by the container, or vice versa.\n",
      "\n",
      "By addressing these limitations and implementing the suggested augmentations, the training data can be significantly improved, leading to a more robust and generalizable robot pick-and-place system. The augmentations, in particular, are designed to be easily implemented with standard image processing techniques.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_summary_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0928f502e3a84925aa109a9fd883abd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyzing Eval Episodes:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_results = analyze_eval_data(eval_repo, episodes=list(range(1,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: \n",
      "\n",
      "Scene Description: [{'point': [591, 500], 'label': 'Robot arm', 'description': 'A red robot arm with black accents, positioned in the center of the image.'}, {'point': [610, 757], 'label': 'container', 'description': 'A blue square container located on the right side of the image.'}, {'point': [727, 301], 'label': 'Lego bricks', 'description': 'A small stack of green and gray Lego bricks, situated on the left side of the image.'}, {'point': [464, 375], 'label': 'Lamp', 'description': 'A black lamp is located on the left side of the image.'}, {'point': [360, 537], 'label': 'Wire', 'description': 'A white wire is connected to the robot arm.'}, {'point': [491, 478], 'label': 'Servo', 'description': 'A black servo motor is attached to the robot arm.'}, {'point': [600, 662], 'label': 'Shadow', 'description': 'The shadow of the container.'}, {'point': [631, 537], 'label': 'Shadow', 'description': 'The shadow of the robot arm.'}, {'point': [760, 287], 'label': 'Shadow', 'description': 'The shadow of the Lego bricks.'}, {'point': [500, 557], 'label': 'Joint', 'description': 'A joint of the robot arm.'}] \n",
      "\n",
      "Episode Evaluation: {'success': True, 'failure_reason': 'N/A'} \n",
      "\n",
      "Episode 1: \n",
      "\n",
      "Scene Description: [{'point': [635, 240], 'label': 'Container', 'description': 'A blue container on the left side of the image.'}, {'point': [470, 504], 'label': 'Robot arm', 'description': 'A red robot arm in the center of the image.'}, {'point': [768, 648], 'label': 'Lego bricks', 'description': 'A stack of green and black Lego bricks on the right side of the image.'}, {'point': [452, 373], 'label': 'Lamp', 'description': 'A lamp behind the container and robot arm.'}, {'point': [347, 550], 'label': 'Wire', 'description': 'A white wire connected to the robot arm.'}, {'point': [400, 654], 'label': 'Table', 'description': 'The table surface behind the robot arm.'}, {'point': [275, 610], 'label': 'Chair', 'description': 'A chair in the background.'}, {'point': [102, 845], 'label': 'Picture', 'description': 'A picture on the wall in the background.'}, {'point': [283, 832], 'label': 'Door', 'description': 'A door in the background.'}, {'point': [693, 348], 'label': 'Table', 'description': 'The table surface in the foreground.'}] \n",
      "\n",
      "Episode Evaluation: {'success': False, 'failure_reason': 'The Lego brick is not visible in the image, so it is not inside the container.'} \n",
      "\n",
      "Episode 2: \n",
      "\n",
      "Scene Description: [{'point': [620, 240], 'label': 'Container', 'description': 'A blue container is on the left side of the table.'}, {'point': [500, 476], 'label': 'Robot arm', 'description': 'A red robot arm is in the center of the table.'}, {'point': [710, 666], 'label': 'Lego bricks', 'description': 'A small Lego brick is on the right side of the table.'}, {'point': [443, 369], 'label': 'Lamp', 'description': 'A lamp is behind the container.'}, {'point': [397, 651], 'label': 'Table', 'description': 'The objects are placed on a white table.'}, {'point': [30, 854], 'label': 'Picture', 'description': 'A picture is hanging on the wall.'}, {'point': [300, 612], 'label': 'Chair', 'description': 'A chair is behind the robot arm.'}] \n",
      "\n",
      "Episode Evaluation: {'success': False, 'failure_reason': 'The Lego brick is not visible in the container.'} \n",
      "\n",
      "Episode 3: \n",
      "\n",
      "Scene Description: [{'point': [691, 320], 'label': 'Lego bricks', 'description': 'A green Lego brick is on the left side of the image.'}, {'point': [500, 503], 'label': 'Robot arm', 'description': 'A red robot arm is in the center of the image.'}, {'point': [608, 742], 'label': 'container', 'description': 'A yellow rectangular container is on the right side of the image.'}, {'point': [458, 368], 'label': 'stand', 'description': 'A black stand is behind the Lego bricks.'}, {'point': [558, 617], 'label': 'wire', 'description': 'A white wire is connected to the robot arm.'}, {'point': [500, 537], 'label': 'sensor', 'description': 'A sensor is on the robot arm.'}, {'point': [591, 795], 'label': 'edge', 'description': 'The edge of the container.'}, {'point': [358, 570], 'label': 'chair', 'description': 'A chair is in the background.'}, {'point': [100, 845], 'label': 'window', 'description': 'A window is in the background.'}] \n",
      "\n",
      "Episode Evaluation: {'success': False, 'failure_reason': 'The Lego brick is not inside the yellow container.'} \n",
      "\n",
      "Episode 4: \n",
      "\n",
      "Scene Description: [{'point': [608, 739], 'label': 'container', 'description': 'A yellow rectangular container is on the right side of the image.'}, {'point': [481, 507], 'label': 'Robot arm', 'description': 'A red robot arm is in the center of the image.'}, {'point': [681, 375], 'label': 'Lego bricks', 'description': 'A green and black Lego brick is to the left of the robot arm.'}, {'point': [387, 368], 'label': 'lamp', 'description': 'A lamp is to the left of the robot arm.'}, {'point': [462, 500], 'label': 'servo', 'description': 'A black servo is mounted on the robot arm.'}, {'point': [581, 654], 'label': 'edge', 'description': 'The edge of the yellow container.'}, {'point': [581, 796], 'label': 'shadow', 'description': 'The shadow of the yellow container.'}, {'point': [375, 525], 'label': 'joint', 'description': 'The joint of the robot arm.'}] \n",
      "\n",
      "Episode Evaluation: {'success': False, 'failure_reason': 'The Lego brick is not inside the container.'} \n",
      "\n",
      "Episode 5: \n",
      "\n",
      "Scene Description: [{'point': [691, 339], 'label': 'Lego bricks', 'description': 'A stack of green Lego bricks is on the table.'}, {'point': [591, 745], 'label': 'Container', 'description': 'A yellow rectangular container is on the table.'}, {'point': [475, 517], 'label': 'Motor', 'description': 'A black motor is mounted on the red robot arm.'}, {'point': [481, 470], 'label': 'Robot arm', 'description': 'A red robot arm is positioned on the table.'}, {'point': [354, 512], 'label': 'Robot arm joint', 'description': 'A joint on the red robot arm is visible.'}, {'point': [581, 535], 'label': 'Robot arm base', 'description': 'The base of the red robot arm is visible.'}] \n",
      "\n",
      "Episode Evaluation: {'success': False, 'failure_reason': 'The Lego brick is not inside the container.'} \n",
      "\n",
      "Episode 6: \n",
      "\n",
      "Scene Description: [{'point': [575, 773], 'label': 'Blue box', 'description': 'A blue box is on the right side of the image.'}, {'point': [698, 250], 'label': 'Lego block', 'description': 'A lego block is on the left side of the image.'}, {'point': [791, 473], 'label': 'Lego block', 'description': 'A lego block is in the center of the image.'}, {'point': [500, 503], 'label': 'Robot arm', 'description': 'A red robot arm is in the center of the image.'}, {'point': [460, 400], 'label': 'Object', 'description': 'A black object is behind the robot arm.'}] \n",
      "\n",
      "Episode Evaluation: {'success': False, 'failure_reason': 'The Lego brick is not inside the blue container.'} \n",
      "\n",
      "Episode 7: \n",
      "\n",
      "Scene Description: [{'point': [785, 429], 'label': 'Lego brick', 'description': 'A small, green and dark green Lego brick sitting on the table.'}, {'point': [485, 500], 'label': 'Robot arm', 'description': 'A red robot arm with black accents, positioned between the Lego brick and the blue box.'}, {'point': [595, 764], 'label': 'Blue box', 'description': 'A blue, square box located to the right of the robot arm.'}, {'point': [445, 376], 'label': 'Lamp', 'description': 'A lamp with a black base and a gray stem, standing behind the robot arm.'}] \n",
      "\n",
      "Episode Evaluation: {'success': True, 'failure_reason': 'N/A'} \n",
      "\n",
      "Episode 8: \n",
      "\n",
      "Scene Description: [{'point': [600, 762], 'label': 'container', 'description': 'A blue square container is on the right side of the image.'}, {'point': [481, 504], 'label': 'Robot arm', 'description': 'A red robot arm is in the center of the image.'}, {'point': [731, 285], 'label': 'Lego bricks', 'description': 'Green and yellow Lego bricks are near the center of the image.'}, {'point': [831, 87], 'label': 'wallet', 'description': 'A black wallet is in the lower left corner of the image.'}, {'point': [789, 159], 'label': 'knife', 'description': 'A red knife is next to the wallet.'}, {'point': [450, 375], 'label': 'lamp', 'description': 'A black lamp is behind the robot arm.'}] \n",
      "\n",
      "Episode Evaluation: {'success': True, 'failure_reason': 'N/A'} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for episode in eval_results:\n",
    "    print(f\"Episode {episode['episode']}: \\n\")\n",
    "    print(f\"Scene Description: {episode['scene_analysis']} \\n\")\n",
    "    print(f\"Episode Evaluation: {episode['episode_eval']} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_suggestions = get_augmentations(eval_results, train_summary_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"recommended_augmentations\": {\n",
      "        \"flip_frame\": true,\n",
      "        \"change_color\": {\n",
      "            \"object\": \"blue container\",\n",
      "            \"target_color\": [\"yellow\", \"green\", \"red\", \"gray\"]\n",
      "        },\n",
      "        \"inpaint_distraction\": {\n",
      "            \"distraction_objects\": [\"wire\", \"shadow\", \"lamp\"]\n",
      "        }\n",
      "    },\n",
      "    \"reasoning\": \"The training data exhibits strong positional and color biases.  The container is always blue and on the right, and the Lego bricks are usually on the left.  The evaluation episodes, while limited, show failures when the container is yellow or when the relative positions of the container and bricks are switched.  Inpainting distractions like the wire and shadows, and in some cases the lamp, will force the model to focus on the key objects (container and bricks).  The successful episodes (0, 7, and 8) all have the blue container on the *right*, reinforcing the positional bias.  The failed episodes often involve a yellow container (3, 4, 5) or a switched position (1, 2, 6).\",\n",
      "    \"expected_improvements\": \"By flipping the frames, we break the positional bias, forcing the model to learn that the container can be on either side.  Changing the container color prevents the model from associating 'container' with only the color blue. Inpainting distractions will improve the model's ability to focus on the relevant objects (Lego bricks and container) regardless of other elements in the scene.  These changes should significantly improve the model's generalization ability and reduce failures in scenarios that deviate from the biased training data.\"\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(augmentation_suggestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_AUG = {'flip_frame': True, 'change_color': {'object': 'blue container', 'target_color': 'blue'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset0 = LeRobotDataset(test_repo, episodes=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = create_dataset(test_repo, dataset0, gsam, DATA_AUG=DATA_AUG)\n",
    "#dataset.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inpaint_flux = modal.Function.lookup('inpaint-flux', 'inpaint_flux', environment_name='prod')\n",
    "#gsam = modal.Function.lookup(\"grounded-sam\",\"GroundedSam.run\", environment_name='prod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caferacer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
