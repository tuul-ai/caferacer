{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First cell - Setup paths\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Setup paths as before\n",
    "notebook_dir = os.path.dirname('__file__')\n",
    "lerobot_root = os.path.abspath(os.path.join(notebook_dir, '../..'))\n",
    "sys.path.append(lerobot_root)\n",
    "os.chdir(lerobot_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[50084]: Class AVFFrameReceiver is implemented in both /Users/shreyas/opt/anaconda3/envs/caferacer/lib/python3.10/site-packages/av/.dylibs/libavdevice.61.3.100.dylib (0x310eec798) and /Users/shreyas/opt/anaconda3/envs/caferacer/lib/libavdevice.59.7.100.dylib (0x325ea4778). One of the two will be used. Which one is undefined.\n",
      "objc[50084]: Class AVFAudioReceiver is implemented in both /Users/shreyas/opt/anaconda3/envs/caferacer/lib/python3.10/site-packages/av/.dylibs/libavdevice.61.3.100.dylib (0x310eec7e8) and /Users/shreyas/opt/anaconda3/envs/caferacer/lib/libavdevice.59.7.100.dylib (0x325ea47c8). One of the two will be used. Which one is undefined.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List, Optional, Dict, Union, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import modal\n",
    "import gc\n",
    "import json\n",
    "from collections import deque\n",
    "import time\n",
    "import re\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset\n",
    "from lerobot.caferacer.scripts.gemini_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_repo = \"shreyasgite/so100_base_left\"\n",
    "eval_repo = \"shreyasgite/eval_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shreyas/opt/anaconda3/envs/caferacer/lib/python3.10/asyncio/events.py:80: DeprecationError: 2025-01-27: `modal.Function.lookup` is deprecated and will be removed in a future release. It can be replaced with `modal.Function.from_name`.\n",
      "\n",
      "See https://modal.com/docs/guide/modal-1-0-migration for more information.\n",
      "  self._context.run(self._callback, *self._args)\n",
      "/Users/shreyas/opt/anaconda3/envs/caferacer/lib/python3.10/asyncio/events.py:80: DeprecationError: 2025-02-11: Looking up class methods using Function.from_name will be deprecated in a future version of Modal.\n",
      "Use modal.Cls.from_name instead, e.g.\n",
      "\n",
      "GroundedSam = modal.Cls.from_name(\"grounded-sam\", \"GroundedSam\")\n",
      "instance = GroundedSam(...)\n",
      "instance.run.remote(...)\n",
      "\n",
      "  self._context.run(self._callback, *self._args)\n"
     ]
    }
   ],
   "source": [
    "gsam = modal.Function.lookup(\"grounded-sam\",\"GroundedSam.run\", environment_name='prod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79000897852148359e93fbbbf0a81232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyzing Training Episodes:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_results = analyze_training_data(train_repo, episodes=list(range(8)))\n",
    "train_summary = summarize_training_data(train_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's an analysis of the provided dataset, focusing on object distributions, success patterns, biases, and limitations, along with suggestions for data augmentation:\n",
      "\n",
      "**1. Training Statistics:**\n",
      "\n",
      "*   **Container Position:** In all 8 episodes (100%), the container is described as being on the *right* side of the image, relative to the robot arm, which is consistently in the center.  There are *no* instances of the container being on the left.\n",
      "*   **Lego Brick Position:** In all 8 episodes (100%), the Lego bricks are on the *left* side of the image, relative to the robot arm. There are *no* instances of the bricks being on the right.\n",
      "*   **Robot Arm Position:** The robot arm is consistently described as being in the \"center\" of the image in all episodes.\n",
      "*   **Lego Brick Colors:** The Lego bricks are consistently described as green, yellow, or dark green.  There's no mention of other Lego colors.\n",
      "*   **Container Color:** The container is always described as \"blue.\" There are no other container colors.\n",
      "*   **Other Objects:**  There are various other objects mentioned, including a \"black tool,\" \"gray object,\" \"door,\" \"table,\" \"antenna,\" \"chair,\" \"lamp,\" and shadows.  These appear inconsistently.\n",
      "*   **Success:** The dataset does not contain information about the success or failure of each pick-and-place episode. There are no labels or descriptions related to task completion.\n",
      "\n",
      "**2. Potential Biases:**\n",
      "\n",
      "*   **Strong Positional Bias:** The most significant bias is the consistent placement of the container on the right and the Lego bricks on the left.  A robot trained solely on this data would likely fail if the positions were reversed.\n",
      "*   **Color Bias:** The Lego bricks are limited to green and yellow variations, and the container is always blue.  This severely limits the robot's ability to generalize to different colored objects.\n",
      "*   **Lack of Negative Examples:**  We don't know if the robot *succeeded* in any of these episodes.  There's no information about failed attempts, dropped bricks, incorrect grasps, etc.  A robust dataset needs negative examples to learn what *not* to do.\n",
      "*   **Environmental Bias:** The background objects (door, table, chair, lamp) and lighting conditions (creating shadows) might introduce unintended correlations.  The robot might learn to rely on these background elements, which would be detrimental in a different environment.\n",
      "*   **Single Camera Viewpoint:** All descriptions imply a single, consistent camera viewpoint.  There's no variation in perspective.\n",
      "\n",
      "**3. Limitations in Dataset Diversity:**\n",
      "\n",
      "*   **Extremely Small Dataset:**  Eight episodes are far too few to train a robust pick-and-place system.  Real-world datasets would have thousands or even millions of examples.\n",
      "*   **No Variation in Brick Configuration:** The descriptions often mention a \"stack\" of bricks, but the size and arrangement of the stack likely have little variation.\n",
      "*   **No Variation in Container Size/Shape:** The container is always described as a \"blue square container.\"  There's no variation in size or shape.\n",
      "*   **No Occlusion:** There's no mention of the Lego bricks being partially or fully occluded (hidden) by the container or other objects.\n",
      "*   **No Robot/Gripper Variation:** The descriptions always refer to a \"red robot arm,\" implying a single robot type and gripper.\n",
      "\n",
      "**4. Suggestions for Data Augmentation:**\n",
      "\n",
      "To address these biases and limitations, the following data augmentation techniques are recommended.  These are categorized for clarity:\n",
      "\n",
      "*   **Geometric Transformations:**\n",
      "\n",
      "    *   `flip_frame: horizontal`: This is *crucial* to address the left/right bias.  Horizontally flipping the images (and adjusting the labels accordingly) would create examples where the container is on the left and the bricks are on the right.\n",
      "    *   `rotate_frame: [90, 180, 270]`: Rotating the image by various degrees (and adjusting labels) would help the robot become invariant to orientation.\n",
      "    *   `translate_objects: [container, lego_bricks]`:  Slightly shifting the positions of the container and Lego bricks within the image would add robustness to small positional variations.  This is different from simply flipping; it introduces small, random shifts.\n",
      "    *   `zoom: [in, out]`: Slightly zooming in and out would help the robot handle variations in object size due to distance.\n",
      "\n",
      "*   **Color and Appearance:**\n",
      "\n",
      "    *   `change_color: object-container: color-[red, green, yellow, white, black, gray]`:  Randomly changing the container's color to a variety of colors is essential.\n",
      "    *   `change_color: object-lego_bricks: color-[red, blue, white, black, gray]`:  Similarly, the Lego bricks should have their colors randomly changed.\n",
      "    *   `adjust_brightness: [0.8, 1.2]`:  Varying the brightness of the entire image simulates different lighting conditions.\n",
      "    *   `adjust_contrast: [0.8, 1.2]`:  Varying the contrast also helps with robustness to lighting.\n",
      "    *   `add_noise: [gaussian, salt_and_pepper]`: Adding small amounts of image noise makes the robot more robust to sensor imperfections.\n",
      "\n",
      "*   **Occlusion and Distraction:**\n",
      "\n",
      "    *   `add_occlusion: object-lego_bricks: occluder-[container, other_objects]`:  Simulate partial occlusion of the Lego bricks by the container or other randomly generated objects.\n",
      "    *   `inpaint_distraction: List_of_distraction_objects:[door, chair, lamp]`: Remove some of the background objects to reduce environmental bias.\n",
      "    *   `add_distraction: List_of_distraction_objects:[random_shapes, random_colored_objects]`: Add *new*, random objects to the background to force the robot to focus on the relevant objects (container and bricks).\n",
      "\n",
      "*   **Simulating Failure:**\n",
      "\n",
      "    *   `simulate_drop: object-lego_bricks`:  Create examples where the Lego bricks are shown as falling or scattered, simulating a failed grasp.  This requires careful labeling to indicate the failed state.\n",
      "    *   `simulate_missed_grasp: object-lego_bricks`: Show the gripper near, but not grasping, the Lego bricks.\n",
      "\n",
      "*   **Dataset Expansion (Beyond Augmentation):**\n",
      "\n",
      "    *   **Collect More Data:** The most important step is to collect *significantly* more data, ideally with real-world variations in object placement, lighting, and background.\n",
      "    *   **Vary Camera Viewpoint:** Capture data from different camera angles and heights.\n",
      "    *   **Introduce Different Containers:** Use containers of different shapes, sizes, and materials.\n",
      "    *   **Vary Lego Brick Configurations:** Use different arrangements and numbers of Lego bricks.\n",
      "    *   **Label Success/Failure:** Explicitly label each episode with whether the pick-and-place operation was successful or not.\n",
      "\n",
      "By implementing these augmentation strategies and expanding the dataset, you can significantly improve the robustness and generalization capabilities of a robot trained for Lego brick pick-and-place tasks. The key is to introduce *controlled* variations that address the identified biases and limitations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ae837f7ce24310a07e1ffe717a348a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyzing Eval Episodes:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_results = analyze_eval_data(eval_repo, episodes=list(range(1,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations_raw = get_augmentations(eval_results, train_summary)\n",
    "augmentations = parse_json(augmentations_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recommended_augmentations': {'flip_frame': True,\n",
       "  'change_color': {'object': 'blue container', 'target_color': 'yellow'},\n",
       "  'inpaint_distraction': {'objects': 'rubik cube'}},\n",
       " 'reasoning': \"The evaluation episodes, combined with the training data analysis, reveal several critical biases that need addressing.  \\n\\n1. **Positional Bias:** The training data consistently placed the container on the right and bricks on the left. While some evaluation episodes had the container on the left (Episode 2) or the brick on the right (Episode 2), the robot consistently failed when the configuration was different from the *dominant* training setup. This strongly suggests the robot overfit to the training data's positional arrangement.  `flip_frame` is essential to correct this.\\n\\n2. **Color Bias:** The training data exclusively used a blue container.  The evaluation introduced yellow containers (Episodes 3, 4, 5), and the robot failed in most cases where the container wasn't blue. This indicates a strong color bias.  `change_color` specifically targeting the container is necessary.\\n\\n3. **Distraction Sensitivity:** The presence of various background objects ('lamp', 'chair', 'window', etc.) in both training and evaluation, and the inconsistent success rates, suggest the robot may be incorrectly associating these objects with the task. The robot needs to learn to ignore irrelevant objects. `inpaint_distraction` on a wide range of these objects is recommended to force the robot to focus on the core task elements (container and bricks).\",\n",
       " 'expected_improvements': \"These augmentations should lead to the following improvements:\\n\\n*   **Improved Generalization:** The robot should be able to successfully pick and place Lego bricks into containers regardless of their position (left or right) and the container's color.\\n*   **Reduced Overfitting:** The robot will be less likely to rely on spurious correlations with background objects or specific positional arrangements.\\n*   **Increased Robustness:** The robot's performance should be more consistent across different environments and in the presence of novel objects.\\n*   **Higher Success Rate:** Overall, the success rate of the pick-and-place task should increase significantly.\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning: The evaluation episodes, combined with the training data analysis, reveal several critical biases that need addressing.  \n",
      "\n",
      "1. **Positional Bias:** The training data consistently placed the container on the right and bricks on the left. While some evaluation episodes had the container on the left (Episode 2) or the brick on the right (Episode 2), the robot consistently failed when the configuration was different from the *dominant* training setup. This strongly suggests the robot overfit to the training data's positional arrangement.  `flip_frame` is essential to correct this.\n",
      "\n",
      "2. **Color Bias:** The training data exclusively used a blue container.  The evaluation introduced yellow containers (Episodes 3, 4, 5), and the robot failed in most cases where the container wasn't blue. This indicates a strong color bias.  `change_color` specifically targeting the container is necessary.\n",
      "\n",
      "3. **Distraction Sensitivity:** The presence of various background objects ('lamp', 'chair', 'window', etc.) in both training and evaluation, and the inconsistent success rates, suggest the robot may be incorrectly associating these objects with the task. The robot needs to learn to ignore irrelevant objects. `inpaint_distraction` on a wide range of these objects is recommended to force the robot to focus on the core task elements (container and bricks).\n",
      "\n",
      "\n",
      "Expected_improvements: These augmentations should lead to the following improvements:\n",
      "\n",
      "*   **Improved Generalization:** The robot should be able to successfully pick and place Lego bricks into containers regardless of their position (left or right) and the container's color.\n",
      "*   **Reduced Overfitting:** The robot will be less likely to rely on spurious correlations with background objects or specific positional arrangements.\n",
      "*   **Increased Robustness:** The robot's performance should be more consistent across different environments and in the presence of novel objects.\n",
      "*   **Higher Success Rate:** Overall, the success rate of the pick-and-place task should increase significantly.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Reasoning: {augmentations['reasoning']}\")\n",
    "print(f\"\\n\\nExpected_improvements: {augmentations['expected_improvements']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'flip_frame': True, 'change_color': {'object': 'blue container', 'target_color': 'yellow'}, 'inpaint_distraction': {'objects': 'rubik cube'}}\n"
     ]
    }
   ],
   "source": [
    "DATA_AUG = augmentations['recommended_augmentations']\n",
    "DATA_AUG['inpaint_distraction']['objects'] = DATA_AUG['inpaint_distraction']['objects'][0]\n",
    "print(DATA_AUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset0 = LeRobotDataset(train_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = create_dataset(train_repo, dataset0, gsam, DATA_AUG=DATA_AUG)\n",
    "#dataset.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caferacer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
